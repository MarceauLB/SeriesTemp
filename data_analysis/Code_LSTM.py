# -*- coding: utf-8 -*-
"""Code_LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17W1WUKnFE_WfNwhiYh8nHiutBMWQcUV3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import sklearn
import torch.nn as nn
from torch.utils.data import Dataset , DataLoader
import yfinance as yf

# Télécharger les données de GOOGL
data = yf.download("GOOGL", start="2007-01-01", end="2018-12-31", interval="1d")

data.head()

data['Log_Return'] = np.log(data['Open'] / data['Open'].shift(1))

# Drop rows with NaN values (the first row will have NaN for returns)
df = data.dropna()

print(df[['Open', 'Log_Return']].head())

"""From now we are going to work with the dataset df and his variable Log_Return"""

df['Log_Return'].plot(figsize=(12,6))
len(df)

df['Open'].plot(figsize=(12,6))

df_rl = df['Log_Return'].astype(float)
print(df_rl) ## Ensure float object

#splitting train/test data
from sklearn.preprocessing import MinMaxScaler
train_data_size = round(0.884*len(df_rl)) ## Ensure to predict the last year 2018
train_data = df_rl[ : train_data_size ]
test_data = df_rl[train_data_size:]
print(len(train_data))
print(len(test_data))

"""Here we will Data Process our data, to be use in our deep model."""

train_data_np = train_data.values
test_data_np = test_data.values

#Standardising features
scaler = MinMaxScaler(feature_range=(-1, 1))
scaler2 = MinMaxScaler(feature_range=(-1, 1))
train_data_normalized = scaler.fit_transform(train_data_np.reshape(-1, 1))
test_data_normalized = scaler2.fit_transform(test_data_np.reshape(-1, 1))

### Transform our data as a Tensor
train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1)
test_data_normalized = torch.FloatTensor(test_data_normalized).view(-1)

## Sliding windows approach (function which creates sequences)
train_window = 100
device = "cuda" if torch.cuda.is_available() else "cpu"
def create_inout_sequences(input_data, tw):
    inout_seq = []
    L = len(input_data)
    for i in range(L-tw):
        train_seq = input_data[i:i+tw]
        train_label = input_data[i+tw:i+tw+1]
        inout_seq.append((train_seq ,train_label))
    return inout_seq

#sequences creation to train our models
train_inout_seq = create_inout_sequences(train_data_normalized, train_window)
test_inout_seq = create_inout_sequences(test_data_normalized, train_window)

#List creation for dataloaders
X_train_tensor = [item[0] for item in train_inout_seq]
y_train_tensor = [item[1] for item in train_inout_seq]

X_test_tensor = [item[0] for item in test_inout_seq]
y_test_tensor = [item[1] for item in test_inout_seq]


print(type(X_train_tensor))
print(len(X_train_tensor[0]))
print(len(X_test_tensor))

from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

dataloader_train = DataLoader(CustomDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True,drop_last=True)
dataloader_test = DataLoader(CustomDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=True,drop_last=True) # Windows independent

"""Create our model"""

from torch import nn


class LSTMNetwork(nn.Module):
    def __init__(self, n_hidden, num_layer):
        super().__init__()
        self.lstm = nn.LSTM(input_size=train_window, hidden_size = n_hidden , num_layers= num_layer)
        self.linear = nn.Linear(n_hidden, 1) ## 1 output days

    def forward(self, x):
        x, (_,_) = self.lstm(x)
        y = self.linear(x)
        return y

#train function to train model
def train_loop(dataloader, model, loss_fn, optimizer, device):

    model.train()
    loss_list=[]
    for X,y in dataloader:
        # Reset gradients
        optimizer.zero_grad()
        # Forward
        pred = model(X.to(device))
        loss = loss_fn(pred,y.to(device))
        # Backpropagation
        loss.backward()
        optimizer.step()
def eval_loop(dataloader, model, loss_fn, device):

    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    loss, correct = 0., 0.

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X.to(device))
            loss += loss_fn(pred, y.to(device)).item()

    loss /= num_batches
    return loss

# Cross validation
from sklearn.model_selection import KFold
import torch.optim as optim
from sklearn.metrics import accuracy_score
import numpy as np


n_splits = 5 #5 blocks Cross validation
skf = KFold(n_splits=n_splits, shuffle=True, random_state=123)

# List of loss
cv_scores = []

epochs =10  #number of epochs to train each model
loss_model_fin=[]
param_list=[]

#parameters to test
list_n_hidden= [16, 32, 64, 128]
list_num_layer= [1,2,3]

loss_fn = nn.MSELoss()

for n_hidden in list_n_hidden:
    for num_layer in list_num_layer:
        loos_model=[]
        for train_index, test_index in skf.split(X_train_tensor):
            X_train, y_train = [], []
            X_val, y_val = [], []
            for elemn_train in train_index :
                X_train.append(X_train_tensor[elemn_train])
                y_train.append(y_train_tensor[elemn_train])
            for elem_test in test_index :
                 X_val.append(X_train_tensor[elem_test])
                 y_val.append( y_train_tensor[elem_test])
            dataloader_train = DataLoader(CustomDataset( X_train,  y_train), batch_size=32, shuffle=True,drop_last=True)
            dataloader_val = DataLoader(CustomDataset( X_val, y_val), batch_size=32, shuffle=True,drop_last=True)
            model = LSTMNetwork(n_hidden,num_layer).to(device) #creation du model à tester
            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
            ##Entrainement du modele
            for t in range(epochs):
                print(f"Epoch {t + 1}\n-------------------------------------------------")
            # Train loop
                train_loop(dataloader_train, model, loss_fn, optimizer, device)
            # Evaluation loop
                loss_train = eval_loop(dataloader_train, model, loss_fn, device)
                loss_val = eval_loop(dataloader_val, model, loss_fn, device)
                loos_model.append(loss_val)

        loss_model_fin.append(np.mean(loos_model))
        param_list.append( [n_hidden, num_layer])

valeur_minimale = np.min(loss_model_fin)
indice_minimale = np.argmin(loss_model_fin)
param_opti = param_list[indice_minimale]
print(valeur_minimale, param_opti)

#validation sample creation
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X_train_tensor, y_train_tensor, test_size=0.3, random_state=42)

# Epochs numbers tuning

dataloader_trainepochs = DataLoader(CustomDataset(X_train, y_train), batch_size=32, shuffle=True,drop_last=True)
dataloader_valepochs = DataLoader(CustomDataset(X_val, y_val), batch_size=32, shuffle=True,drop_last=True)

model = LSTMNetwork(32,3).to(device) ## With our hyperparameters find above

epochs =300
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
losslist_test= []
losslist_train=[]


for t in range(epochs):
    print(f"Epoch {t + 1}\n-------------------------------------------------")

    # Train loop
    train_loop(dataloader_trainepochs, model, loss_fn, optimizer, device)

    # Evaluation loop
    loss_train = eval_loop(dataloader_trainepochs, model, loss_fn, device)
    losslist_train.append(loss_train)

    print(f"Training set: Avg loss: {loss_train: >8f}")

    loss_test = eval_loop(dataloader_valepochs, model, loss_fn, device)
    losslist_test.append(loss_test)
    print(f"Val set:  Avg loss: {loss_test: >8f}\n")

print("Done!")

#Loss versus epochs graphic
plt.plot([i for i in range(epochs)],losslist_train,color='blue')
plt.plot([i for i in range(epochs)],losslist_test,color='red')

plt.axvline(x=60 , color='red', linestyle='--', label='Vertical Line at x=25')

plt.show()

#training the final model on the whole train set.
dataloader_train = DataLoader(CustomDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True,drop_last=True)
dataloader_test = DataLoader(CustomDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=True,drop_last=True)

model = LSTMNetwork(32,3).to(device)

epochs = 60
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for t in range(epochs):
    print(f"Epoch {t + 1}\n-------------------------------------------------")

    # Train loop
    train_loop(dataloader_train, model, loss_fn, optimizer, device)

    # Evaluation loop
    loss_train = eval_loop(dataloader_train, model, loss_fn, device)

    print(f"Training set: Avg loss: {loss_train: >8f}")

    loss_test = eval_loop(dataloader_test, model, loss_fn, device)
    print(f"Test set:  Avg loss: {loss_test: >8f}\n")

print("Done!")

"""Predictions on train set"""

#compute prediction on train set
pred_arr = []
y_arr = []
model.eval()
train_dataloader = DataLoader(CustomDataset(X_train_tensor, y_train_tensor), batch_size=8,drop_last=True)
with torch.no_grad():
    for batch , item in enumerate(train_dataloader):
        x , y = item
        x , y = x.to(device) , y.to(device)
        print(x)
        pred = model(x)
        pred = scaler.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)
        y = scaler.inverse_transform(y.detach().cpu().numpy()).reshape(-1)
        pred_arr = pred_arr + list(pred)
        y_arr = y_arr + list(y)

#function to compute rmse on the log return of the train set.
import math
from sklearn.metrics import mean_squared_error
import numpy as np
def calculate_metrics_train(data_loader):
    pred_arr = []
    y_arr = []
    with torch.no_grad():
        for batch , item in enumerate(data_loader):
            x , y = item
            x , y = x.to(device) , y.to(device)
            pred = model(x)
            pred = scaler.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)
            y = scaler.inverse_transform(y.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
            pred_arr = pred_arr + list(pred)
            y_arr = y_arr + list(y)
        return math.sqrt(mean_squared_error(y_arr,pred_arr))

#function to compute rmse on the log return of the test set.
import math
from sklearn.metrics import mean_squared_error
import numpy as np
def calculate_metrics_test(data_loader):
    pred_arr = []
    y_arr = []
    with torch.no_grad():
        for batch , item in enumerate(data_loader):
            x , y = item
            x , y = x.to(device) , y.to(device)
            pred = model(x)
            pred = scaler2.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)
            y = scaler2.inverse_transform(y.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
            pred_arr = pred_arr + list(pred)
            y_arr = y_arr + list(y)
        return math.sqrt(mean_squared_error(y_arr,pred_arr))

#Loss on train and test set
print(f"train mse loss {calculate_metrics_train(dataloader_train)}")
print(f"test mse loss {calculate_metrics_test(dataloader_test)}")

# fitted train values
plt.plot([i for i in range(len(pred_arr))],y_arr,color='blue')
plt.plot([i for i in range(len(pred_arr))],pred_arr,color='orange')
plt.gcf().set_size_inches(15, 5)

plt.show()

"""Prediction on test set, using real values as input"""

pred_arr_test = []
y_arr_test = []
dataloader_test = DataLoader(CustomDataset(X_test_tensor, y_test_tensor), batch_size=8,drop_last=True)
with torch.no_grad():
    for batch , item in enumerate(dataloader_test):
        x , y = item
        x , y = x.to(device) , y.to(device)
        pred = model(x)
        pred = scaler2.inverse_transform(pred.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
        y = scaler2.inverse_transform(y.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
        pred_arr_test = pred_arr_test + list(pred)
        y_arr_test = y_arr_test + list(y)

#fitted values not using its own predictions but true values
plt.plot([i for i in range(len(pred_arr_test))],y_arr_test,color='blue')
plt.plot([i for i in range(len(pred_arr_test))],pred_arr_test,color='orange')
plt.gcf().set_size_inches(15, 5)
plt.show()

"""Prediction on test set, using previous prediction as input"""

pred_list_test=[]
print(len(test_data))
pred_data_nump= test_data_normalized[0:train_window ]
print(len(pred_data_nump))
nbr_pred=365
for i in range(nbr_pred):
    pred_data_nump=np.append(pred_data_nump,0)
    pred_data= torch.FloatTensor(pred_data_nump).view(-1)
    pred_inout_seq = create_inout_sequences(pred_data, train_window )
    pred_tensor = [item[0] for item in pred_inout_seq]
    y_pred = [item[1] for item in pred_inout_seq]
    dataloader_pred = DataLoader(CustomDataset(pred_tensor,y_pred),batch_size=1)
    model.eval()
    with torch.no_grad():
        for i,j in dataloader_pred:
            pred= model(i.to(device))
            pred_data_nump= np.append(pred_data_nump[1:len(pred_data_nump)-1],pred.detach().cpu().numpy().reshape(1,-1))
            pred = scaler2.inverse_transform(pred.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
            pred_list_test= pred_list_test + list(pred)
print(len(pred_list_test))

#Predicted values on test set using its own predictions for 365 values

plt.plot([i for i in range(len(pred_list_test))],pred_list_test,color='orange')
plt.plot([i for i in range(nbr_pred)],test_data[(train_window):(train_window+nbr_pred)],color='blue')

plt.gcf().set_size_inches(15, 5)
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error

# RMSE
rmse = math.sqrt(mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)]))

# MSE
mse = mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

# MAE
mae = mean_absolute_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

##compute fitted values on test set using its own predictions at each step
pred_list_test=[]
print(len(test_data))
pred_data_nump= test_data_normalized[0:train_window ]
print(len(pred_data_nump))
nbr_pred=5
for i in range(nbr_pred):
    pred_data_nump=np.append(pred_data_nump,0)
    pred_data= torch.FloatTensor(pred_data_nump).view(-1)
    pred_inout_seq = create_inout_sequences(pred_data, train_window )
    pred_tensor = [item[0] for item in pred_inout_seq]
    y_pred = [item[1] for item in pred_inout_seq]
    dataloader_pred = DataLoader(CustomDataset(pred_tensor,y_pred),batch_size=1)
    model.eval()
    with torch.no_grad():
        for i,j in dataloader_pred: #le dataloarder est de longueur 1
            pred= model(i.to(device))
            pred_data_nump= np.append(pred_data_nump[1:len(pred_data_nump)-1],pred.detach().cpu().numpy().reshape(1,-1))
            pred = scaler2.inverse_transform(pred.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
            pred_list_test= pred_list_test + list(pred)
print(len(pred_list_test))
pred_5_lstm_py = pred_list_test

#Predicted values on test set using its own predictions for 5 values

plt.plot([i for i in range(len(pred_list_test))],pred_list_test,color='orange')
plt.plot([i for i in range(nbr_pred)],test_data[(train_window):(train_window+nbr_pred)],color='blue')

plt.gcf().set_size_inches(15, 5)
plt.show()

# RMSE
rmse = math.sqrt(mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)]))

# MSE
mse = mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

# MAE
mae = mean_absolute_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

##compute fitted values on test set using its own predictions at each step
pred_list_test=[]
print(len(test_data))
pred_data_nump= test_data_normalized[0:train_window ]
print(len(pred_data_nump))
nbr_pred=22
for i in range(nbr_pred):
    pred_data_nump=np.append(pred_data_nump,0)
    pred_data= torch.FloatTensor(pred_data_nump).view(-1)
    pred_inout_seq = create_inout_sequences(pred_data, train_window )
    pred_tensor = [item[0] for item in pred_inout_seq]
    y_pred = [item[1] for item in pred_inout_seq]
    dataloader_pred = DataLoader(CustomDataset(pred_tensor,y_pred),batch_size=1)
    model.eval()
    with torch.no_grad():
        for i,j in dataloader_pred: #le dataloarder est de longueur 1
            pred= model(i.to(device))
            pred_data_nump= np.append(pred_data_nump[1:len(pred_data_nump)-1],pred.detach().cpu().numpy().reshape(1,-1))
            pred = scaler2.inverse_transform(pred.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
            pred_list_test= pred_list_test + list(pred)
print(len(pred_list_test))
pred_22_lstm_py = pred_list_test

#Predicted values on test set using its own predictions for 22 values

plt.plot([i for i in range(len(pred_list_test))],pred_list_test,color='orange')
plt.plot([i for i in range(nbr_pred)],test_data[(train_window):(train_window+nbr_pred)],color='blue')

plt.gcf().set_size_inches(15, 5)
plt.show()

# RMSE
rmse = math.sqrt(mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)]))

# MSE
mse = mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

# MAE
mae = mean_absolute_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

##compute fitted values on test set using its own predictions at each step
pred_list_test=[]
print(len(test_data))
pred_data_nump= test_data_normalized[0:train_window ]
print(len(pred_data_nump))
nbr_pred=250
for i in range(nbr_pred):
    pred_data_nump=np.append(pred_data_nump,0)
    pred_data= torch.FloatTensor(pred_data_nump).view(-1)
    pred_inout_seq = create_inout_sequences(pred_data, train_window )
    pred_tensor = [item[0] for item in pred_inout_seq]
    y_pred = [item[1] for item in pred_inout_seq]
    dataloader_pred = DataLoader(CustomDataset(pred_tensor,y_pred),batch_size=1)
    model.eval()
    with torch.no_grad():
        for i,j in dataloader_pred: #le dataloarder est de longueur 1
            pred= model(i.to(device))
            pred_data_nump= np.append(pred_data_nump[1:len(pred_data_nump)-1],pred.detach().cpu().numpy().reshape(1,-1))
            pred = scaler2.inverse_transform(pred.detach().cpu().numpy().reshape(1,-1)).reshape(-1)
            pred_list_test= pred_list_test + list(pred)
print(len(pred_list_test))
pred_250_lstm_py = pred_list_test

#Predicted values on test set using its own predictions for 250 values

plt.plot([i for i in range(len(pred_list_test))],pred_list_test,color='orange')
plt.plot([i for i in range(nbr_pred)],test_data[(train_window):(train_window+nbr_pred)],color='blue')

plt.gcf().set_size_inches(15, 5)
plt.show()

# RMSE
rmse = math.sqrt(mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)]))

# MSE
mse = mean_squared_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

# MAE
mae = mean_absolute_error(pred_list_test, test_data[(train_window):(train_window+nbr_pred)])

print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

"""IF log return use"""

#function to return the true initial values
data_vraie = df["Open"].values
def log_returns_to_prices(log_returns):
    # Calculer les valeurs originales à partir des rendements logarithmiques
    vrai_prix = []
    cumulative_returns = np.cumsum(log_returns)
    prices= np.exp(cumulative_returns)
    x = data_vraie[train_data_size+train_window-1]
    prices= x*prices
    return prices

pred5 = log_returns_to_prices(pred_5_lstm_py)
pred22 = log_returns_to_prices(pred_22_lstm_py)
pred250 = log_returns_to_prices(pred_250_lstm_py)

vrai_prix = log_returns_to_prices(test_data[train_window:250+train_window])

plt.plot([i for i in range(len(pred250))],pred250,color='orange')
plt.plot([i for i in range(len(vrai_prix))],vrai_prix,color='blue')
plt.plot([i for i in range(len(vrai_prix))],data_vraie[train_data_size+train_window:train_data_size+train_window+nbr_pred],color='blue')

# RMSE
rmse = math.sqrt(mean_squared_error(pred250, vrai_prix))

# MSE
mse = mean_squared_error(pred250, vrai_prix)

# MAE
mae = mean_absolute_error(pred250, vrai_prix)

print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

import pandas as pd
import numpy as np

# Assuming pred_list_test contains the prediction values
# Fill with NA if the length is less than 250
required_length = 250

# Fill each prediction list to length 250
pred5_lstm = np.append(pred5, [np.nan] * (required_length - len(pred5)))
pred22_lstm = np.append(pred22, [np.nan] * (required_length - len(pred22)))
pred250_lstm = np.append(pred250, [np.nan] * (required_length - len(pred250)))

# Create a DataFrame with the formatted predictions
create_data = pd.DataFrame({
    'pred5': pred5_lstm,
    'pred22': pred22_lstm,
    'pred250': pred250_lstm
})

create_data.to_csv('forecast_predictions_lstm.csv', index=False)

from google.colab import files
files.download('forecast_predictions_lstm.csv')

"""Encoder Decoder"""